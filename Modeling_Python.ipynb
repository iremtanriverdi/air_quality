{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46b530-7ad5-49f0-8579-ba60cc3c2c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95c1e5-7be4-4dc0-b24b-d8ea968051cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "ds = xr.open_dataset(\"df_final.nc\")\n",
    "df = ds.to_dataframe().reset_index()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dae7e-80a1-4812-a3a1-8ee52cf0e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "train_df = df[df[\"timestamp\"] < \"2023-01-01\"]  # 2013-2022  tarining\n",
    "test_df = df[df[\"timestamp\"] >= \"2023-01-01\"]  # 2023  test\n",
    "\n",
    "\n",
    "target_column = \"PCA_AQI\" \n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_column in numerical_features:\n",
    "    numerical_features.remove(target_column)\n",
    "\n",
    "excluded_features = [\"carbon_monoxide\", \"PM2.5_ugm3\", \"ozone\", \"PM10_ugm3\", \n",
    "                     \"nitrogen_dioxide\", \"sulfur_dioxide\"]\n",
    "\n",
    "\n",
    "# Ensure numerical_features is a list of column names\n",
    "numerical_features = [col for col in numerical_features if col not in excluded_features]\n",
    "\n",
    "train_df[numerical_features] = train_df[numerical_features].fillna(train_df[numerical_features].mean())\n",
    "test_df[numerical_features] = test_df[numerical_features].fillna(test_df[numerical_features].mean())\n",
    "\n",
    "X_train = train_df[numerical_features]\n",
    "y_train = train_df[target_column]\n",
    "X_test = test_df[numerical_features]\n",
    "y_test = test_df[target_column]\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82966e46-58ea-40e9-a395-388d53f7414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train DataFrame shape:\", train_df.shape)\n",
    "print(\"Test DataFrame shape:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f3ab4b-5e3b-4e1e-b5d8-2c15cb9b8c5c",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19743b0-e7cd-479a-879b-9c04b13d9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.05, \n",
    "    \"num_leaves\": 31,\n",
    "    \"max_depth\": -1,\n",
    "    \"feature_fraction\": 0.8,  \n",
    "    \"bagging_fraction\": 0.8,  \n",
    "    \"bagging_freq\": 5,  \n",
    "    \"verbose\": -1,\n",
    "    \"n_jobs\": -1,  \n",
    "}\n",
    "\n",
    "\n",
    "model = lgb.train(params, lgb_train, num_boost_round=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3be30f-c32e-45d2-abc2-ade99c2758a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance \n",
    "import numpy as np\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns, \n",
    "    'Importance': model.feature_importance(importance_type='gain')  # 'gain' \n",
    "})\n",
    "\n",
    "selected_features_df = feature_importance.nlargest(30, \"Importance\")\n",
    "TOP_FEATURES = selected_features_df[\"Feature\"].tolist()\n",
    "TOP_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc5a14-6cdd-4f39-be87-ae7c39187682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP \n",
    "explainer = shap.Explainer(model, X_train[TOP_FEATURES])\n",
    "shap_values = explainer(X_test[TOP_FEATURES])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance[\"Feature\"][:30], feature_importance[\"Importance\"][:30])\n",
    "plt.xlabel(\"Feature Importance (LightGBM)\")\n",
    "plt.title(\"Top 30 Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "shap.summary_plot(shap_values, X_test[TOP_FEATURES])\n",
    "\n",
    "\n",
    "selected_features_df = feature_importance.head(30)\n",
    "selected_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1907ce-8bbe-4212-a45a-d79e7bdd170a",
   "metadata": {},
   "source": [
    "### Mixed-Effect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86191fc4-6b7f-47c5-bc03-d698d38008a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import mixedlm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = numerical_features + [\"PCA_AQI\"]  # PCA_AQI de scale edilmeli\n",
    "df[scaled_features] = scaler.fit_transform(df[scaled_features])\n",
    "\n",
    "\n",
    "train_df = df[df[\"timestamp\"] < \"2023-01-01\"]\n",
    "test_df = df[df[\"timestamp\"] >= \"2023-01-01\"]\n",
    "\n",
    "\n",
    "\n",
    "train_df['date'] = pd.to_datetime(train_df['timestamp']).dt.to_period('M')  # AylÄ±k formatta tarih\n",
    "train_df['year'] = train_df['date'].dt.year\n",
    "train_df['month'] = train_df['date'].dt.month\n",
    "\n",
    "test_df['date'] = pd.to_datetime(test_df['timestamp']).dt.to_period('M')  # AylÄ±k formatta tarih\n",
    "test_df['year'] = test_df['date'].dt.year\n",
    "test_df['month'] = test_df['date'].dt.month\n",
    "\n",
    "formula = \"PCA_AQI ~ temperature_2m_C_y + wind_speed_10m_ms + surface_pressure_hPa_y + total_precipitation_mm + \\\n",
    "total_column_ozone_y + surface_solar_radiation_downward_Wm2 + sea_salt_aerosol_1 + organic_matter_aerosol_2 + \\\n",
    "nitric_oxide + methane + specific_humidity_kgkg + boundary_layer_height_m + \\\n",
    "mean_sea_level_pressure_hPa + wind_u_component_10m_ms + wind_v_component_10m_ms + year + month + \\\n",
    "low_cloud_cover_percent + evaporation_mm + ethane + formaldehyde  + potential_vorticity_Km2s + relative_humidity_percent + vertical_velocity_Pas\"\n",
    "\n",
    "md = mixedlm(formula, train_df, groups=train_df[\"Country\"])  # re_formula eklemedik, date random slope olmadÄ±\n",
    "mdf = md.fit(method='lbfgs')\n",
    "print(mdf.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35065e1-2c88-4bf0-af78-06d582f508b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random effects param.\n",
    "print(mdf.random_effects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51842dda-2ffe-4d16-93ec-9e8bf9f94e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Mixed Effects Model\n",
    "md = sm.MixedLM.from_formula(formula, train_df, groups=train_df[\"Country\"])\n",
    "mdf = md.fit(method='lbfgs')\n",
    "\n",
    "#  Generate Predictions on the Test Set\n",
    "y_test = test_df[\"PCA_AQI\"]  # True values\n",
    "y_pred = mdf.predict(test_df)  # Model predictions\n",
    "\n",
    "#  Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "#  Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "#  Calculate Adjusted RÂ²\n",
    "n = len(y_test)  # Number of samples\n",
    "p = len(mdf.params) - 1  # Number of predictors (excluding intercept)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "adj_r2 = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n",
    "\n",
    "\n",
    "print(f\" MAE: {mae:.4f}\")\n",
    "print(f\" RMSE: {rmse:.4f}\")\n",
    "print(f\" Adjusted RÂ²: {adj_r2:.4f}\")\n",
    "\n",
    "print(mdf.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f62ab0-bc03-454d-9bb5-8359fb1fb076",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bdbeab-1f3b-4723-8c81-857ccefe9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = [\"temperature_2m_C_y\", \"wind_speed_10m_ms\", \"surface_pressure_hPa_y\",\n",
    "            \"total_precipitation_mm\", \"total_column_ozone_y\", \"surface_solar_radiation_downward_Wm2\",\n",
    "            \"sea_salt_aerosol_1\", \"organic_matter_aerosol_2\", \"nitric_oxide\", \"methane\",\n",
    "            \"specific_humidity_kgkg\", \"boundary_layer_height_m\", \"mean_sea_level_pressure_hPa\",\n",
    "            \"wind_u_component_10m_ms\", \"wind_v_component_10m_ms\", \"year\", \"month\",\n",
    "            \"low_cloud_cover_percent\", \"evaporation_mm\", \"ethane\", \"formaldehyde\",\n",
    "            \"potential_vorticity_Km2s\", \"relative_humidity_percent\", \"vertical_velocity_Pas\"]\n",
    "\n",
    "X_train = train_df[features].values\n",
    "X_test = test_df[features].values\n",
    "y_train = train_df[\"PCA_AQI\"].values\n",
    "y_test = test_df[\"PCA_AQI\"].values\n",
    "\n",
    "#  Create Sequences for LSTM (Lookback: 30 days)\n",
    "lookback = 90  \n",
    "\n",
    "def create_sequences(X, y, lookback):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X)):  # Start from `lookback` to align properly\n",
    "        Xs.append(X[i - lookback : i])  # Take `lookback` previous time steps\n",
    "        ys.append(y[i])  # Target is at time `i`\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "#  Create Properly Aligned Sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, lookback)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, lookback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f314cc-d00b-424a-934b-0aa4752059e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='relu', return_sequences=True, kernel_regularizer=l2(0.01), input_shape=(lookback, len(features))),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    LSTM(64, activation='relu', return_sequences=False, kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(1)  # Output layer for PCA_AQI prediction\n",
    "])\n",
    "\n",
    "#  Compile Model with Learning Rate Scheduler\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "#  Learning Rate Scheduler (Reduces LR if no improvement)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, verbose=1)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=200, batch_size=32, \n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"LSTM Training Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d088d-3c1e-4e74-9900-f9a6c8e9064b",
   "metadata": {},
   "source": [
    "### H-MED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8fdc42-ffe8-4799-809b-8a89970550e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from statsmodels.formula.api import mixedlm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "features = [\n",
    "    \"temperature_2m_C_y\", \"wind_speed_10m_ms\", \"surface_pressure_hPa_y\",\n",
    "    \"total_precipitation_mm\", \"total_column_ozone_y\", \"surface_solar_radiation_downward_Wm2\",\n",
    "    \"sea_salt_aerosol_1\", \"organic_matter_aerosol_2\", \"nitric_oxide\", \"methane\",\n",
    "    \"specific_humidity_kgkg\", \"boundary_layer_height_m\", \"mean_sea_level_pressure_hPa\",\n",
    "    \"wind_u_component_10m_ms\", \"wind_v_component_10m_ms\", \"low_cloud_cover_percent\",\n",
    "    \"evaporation_mm\", \"ethane\", \"formaldehyde\", \"potential_vorticity_Km2s\",\n",
    "    \"relative_humidity_percent\", \"vertical_velocity_Pas\", \"lat\", \"lon\"\n",
    "]\n",
    "target = \"PCA_AQI\"\n",
    "\n",
    "#  3. Train-Test Split\n",
    "train_df = df[df[\"timestamp\"] < \"2023-01-01\"].copy()\n",
    "test_df = df[df[\"timestamp\"] >= \"2023-01-01\"].copy()\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_df[features + [target]] = scaler.fit_transform(train_df[features + [target]])\n",
    "test_df[features + [target]] = scaler.transform(test_df[features + [target]])\n",
    "\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_[\"year\"] = pd.to_datetime(df_[\"timestamp\"]).dt.year\n",
    "    df_[\"month\"] = pd.to_datetime(df_[\"timestamp\"]).dt.month\n",
    "\n",
    "features += [\"year\", \"month\"]\n",
    "\n",
    "#  Mixed-Effect Model: Random Intercept \n",
    "formula = \"PCA_AQI ~ \" + \" + \".join(features)\n",
    "md = mixedlm(formula, train_df, groups=train_df[\"Country\"])\n",
    "mdf = md.fit(method='lbfgs')\n",
    "\n",
    "random_intercepts = {k: float(v.values[0]) if isinstance(v, pd.Series) else float(v) for k, v in mdf.random_effects.items()}\n",
    "train_df[\"random_intercept\"] = train_df[\"Country\"].map(random_intercepts).astype(float)\n",
    "test_df[\"random_intercept\"] = test_df[\"Country\"].map(random_intercepts).astype(float)\n",
    "\n",
    "# Gaussian Process Regression (GPR) \n",
    "gpr_sample_size = 5000  \n",
    "sampled_data = train_df.sample(n=gpr_sample_size, random_state=42)\n",
    "\n",
    "kernel = C(1.0) * RBF(length_scale=1.0)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "gpr.fit(sampled_data[[\"lat\", \"lon\"]], sampled_data[target])\n",
    "\n",
    "train_df[\"gpr_prediction\"] = gpr.predict(train_df[[\"lat\", \"lon\"]])\n",
    "test_df[\"gpr_prediction\"] = gpr.predict(test_df[[\"lat\", \"lon\"]])\n",
    "features += [\"gpr_prediction\"]\n",
    "\n",
    "\n",
    "def to_tensor(df):\n",
    "    X = torch.tensor(df[features].values, dtype=torch.float32).to(device)\n",
    "    lat_lon = torch.tensor(df[[\"lat\", \"lon\"]].values, dtype=torch.float32).to(device)\n",
    "    year_month = torch.tensor(df[[\"year\", \"month\"]].values, dtype=torch.float32).to(device)\n",
    "    intercept = torch.tensor(df[\"random_intercept\"].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    return torch.cat((X, intercept), dim=1), lat_lon, year_month\n",
    "\n",
    "X_train, lat_lon_train, year_month_train = to_tensor(train_df)\n",
    "X_test, lat_lon_test, year_month_test = to_tensor(test_df)\n",
    "y_train = torch.tensor(train_df[target].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_test = torch.tensor(test_df[target].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 16 \n",
    "embed_dim = 4  \n",
    "\n",
    "#  LSTM Diffusion \n",
    "class SpatioTemporalLSTMDiffusion(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, embed_dim):\n",
    "        super(SpatioTemporalLSTMDiffusion, self).__init__()\n",
    "\n",
    "        self.spatial_embedding = nn.Linear(2, embed_dim)\n",
    "        self.temporal_embedding = nn.Linear(2, embed_dim)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + embed_dim * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_dim)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(latent_dim, latent_dim, num_layers=1, batch_first=True)  # ðŸ”¥ Katman sayÄ±sÄ± azaltÄ±ldÄ±\n",
    "\n",
    "        self.noise_scale = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lat_lon, year_month, t):\n",
    "        spatial_embed = self.spatial_embedding(lat_lon)\n",
    "        temporal_embed = self.temporal_embedding(year_month)\n",
    "\n",
    "        x = torch.cat([x, spatial_embed, temporal_embed], dim=1)\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        z, _ = self.lstm(z.unsqueeze(1))  \n",
    "        z = z.squeeze(1)  \n",
    "\n",
    "        noise = torch.randn_like(z) * self.noise_scale * t\n",
    "        z_noisy = z + noise  \n",
    "\n",
    "        out = self.decoder(z_noisy)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = SpatioTemporalLSTMDiffusion(input_dim, latent_dim, embed_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "batch_size = 512 \n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        x_batch = X_train[i:i+batch_size]\n",
    "        lat_lon_batch = lat_lon_train[i:i+batch_size]\n",
    "        year_month_batch = year_month_train[i:i+batch_size]\n",
    "        time_step = torch.rand(x_batch.shape[0], 1).to(device)\n",
    "        \n",
    "        y_pred = model(x_batch, lat_lon_batch, year_month_batch, time_step)\n",
    "        loss = loss_fn(y_pred, y_train[i:i+batch_size])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09c491-a813-4b14-bd3d-093d9c902fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    time_step = torch.rand(X_test.shape[0], 1).to(device)  \n",
    "    y_pred_test = model(X_test, lat_lon_test, year_month_test, time_step)\n",
    "\n",
    "\n",
    "y_test_np = y_test.cpu().numpy().flatten()\n",
    "y_pred_np = y_pred_test.cpu().numpy().flatten()\n",
    "\n",
    "#  Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test_np, y_pred_np)\n",
    "\n",
    "#  Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_np))\n",
    "\n",
    "#  Adjusted RÂ² \n",
    "def adjusted_r2(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "n = X_test.shape[0]  \n",
    "p = X_test.shape[1]  \n",
    "adj_r2 = adjusted_r2(y_test_np, y_pred_np, n, p)\n",
    "\n",
    "\n",
    "print(f\"Test MAE: {mae:.4f}\")\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test Adjusted RÂ²: {adj_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7077e8-5f4f-4d50-b154-ecefe0b2a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "# Step 1: Mixed-Effect Model - Random Intercept Calculation\n",
    "formula = f\"{target} ~ \" + \" + \".join(features)\n",
    "md = mixedlm(formula, train_df, groups=train_df[\"Country\"])\n",
    "mdf = md.fit()\n",
    "\n",
    "# Extract Random Intercepts\n",
    "random_intercepts = {k: float(v.values[0]) if isinstance(v, pd.Series) else float(v) for k, v in mdf.random_effects.items()}\n",
    "train_df[\"random_intercept\"] = train_df[\"Country\"].map(random_intercepts).astype(float)\n",
    "test_df[\"random_intercept\"] = test_df[\"Country\"].map(random_intercepts).astype(float)\n",
    "\n",
    "# Convert Data to Tensors\n",
    "def to_tensor(df):\n",
    "    X = torch.tensor(df[features].values, dtype=torch.float32).to(device)\n",
    "    random_intercept = torch.tensor(df[\"random_intercept\"].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    return torch.cat((X, random_intercept), dim=1)\n",
    "\n",
    "X_train = to_tensor(train_df)\n",
    "X_test = to_tensor(test_df)\n",
    "y_train = torch.tensor(train_df[target].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_test = torch.tensor(test_df[target].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Model Parameters\n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 32  \n",
    "hidden_dim = 64  \n",
    "embed_dim = 8  \n",
    "batch_size = 512  \n",
    "learning_rate = 0.001  \n",
    "\n",
    "#  Step 2: Hypernetwork\n",
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "#  Step 3: LSTM Forecaster\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, weights):\n",
    "        x, _ = self.lstm(x)  \n",
    "        x = self.fc(x[:, -1, :])  \n",
    "        return x\n",
    "\n",
    "#  Step 4: Full HITS Model\n",
    "class HITSModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(HITSModel, self).__init__()\n",
    "        self.hypernetwork = HyperNetwork(input_dim, hidden_dim, latent_dim)\n",
    "        self.forecaster = LSTMForecaster(latent_dim, hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.hypernetwork(x)  \n",
    "        x = x.unsqueeze(1).repeat(1, 10, 1)  # Expand sequence dimension for LSTM\n",
    "        output = self.forecaster(x, weights)  \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26b322-050a-4b59-96fa-df978ef5653d",
   "metadata": {},
   "source": [
    "### HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce6d37-16f4-4322-9df0-54a8491d5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "\n",
    "\n",
    "def to_tensor(df):\n",
    "    X = torch.tensor(df[features].values, dtype=torch.float32).to(device)\n",
    "    random_intercept = torch.tensor(df[\"random_intercept\"].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    return torch.cat((X, random_intercept), dim=1)\n",
    "\n",
    "X_train = to_tensor(train_df)\n",
    "X_test = to_tensor(test_df)\n",
    "y_train = torch.tensor(train_df[target].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_test = torch.tensor(test_df[target].values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "#  Model Parameters\n",
    "input_dim = X_train.shape[1]  # Should match Hypernetwork output\n",
    "latent_dim = input_dim  # Ensure LSTM input matches Hypernetwork output\n",
    "hidden_dim = 64  \n",
    "embed_dim = 8  \n",
    "sequence_length = 10  # Define sequence length for LSTM\n",
    "batch_size = 512  \n",
    "learning_rate = 0.001  \n",
    "\n",
    "#  Hypernetwork\n",
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)  # Output must match input_dim of LSTM\n",
    "\n",
    "#  LSTM Forecaster\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)  # LSTM expects (batch_size, seq_len, input_dim)\n",
    "        x = self.fc(x[:, -1, :])  # Use only last output step\n",
    "        return x\n",
    "\n",
    "#  Full HITS Model\n",
    "class HITSModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(HITSModel, self).__init__()\n",
    "        self.hypernetwork = HyperNetwork(input_dim, hidden_dim, latent_dim)\n",
    "        self.forecaster = LSTMForecaster(latent_dim, hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.hypernetwork(x)  \n",
    "        \n",
    "        # âœ… Reshape to match LSTM input (batch_size, seq_length, input_dim)\n",
    "        x = x.unsqueeze(1).repeat(1, sequence_length, 1)  \n",
    "        \n",
    "        output = self.forecaster(x)  \n",
    "        return output\n",
    "\n",
    "#  Initialize Model\n",
    "model = HITSModel(input_dim, hidden_dim, latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "#  Training Loop (With Mini-Batches)\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        x_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "#  Model Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "\n",
    "#  Performance Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "y_test_np = y_test.cpu().numpy().flatten()\n",
    "y_pred_np = y_pred_test.cpu().numpy().flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_test_np, y_pred_np)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_np))\n",
    "r2 = r2_score(y_test_np, y_pred_np)\n",
    "\n",
    "print(f\"Test MAE: {mae:.4f}\")\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test RÂ²: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
